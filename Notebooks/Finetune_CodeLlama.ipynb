{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5rfduRD1CSF"
      },
      "source": [
        "# Code Llama Finetune\n",
        "\n",
        "**In this notebook, I'll show you how to fine-tune Code Llama to become a C++ code generation beast. For coding tasks, we can generally get better performance from Code Llama than Llama 2, especially when we specialize the model to a specific code generation task: **\n",
        "\n",
        "- Finetune with [Lora](https://github.com/microsoft/LoRA) method，Quantize the base model to int 8, freeze its weights, and only train the adapter\n",
        "- The code is referenced from [alpaca-lora](https://github.com/tloen/alpaca-lora)，and the certain improvements and optimizations have been made at the same time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFQ9XeBI1CSG"
      },
      "source": [
        "### 1. Install and load necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Y0x4OWfq1CSG"
      },
      "outputs": [],
      "source": [
        "# !pip install git+https://github.com/huggingface/transformers.git@main bitsandbytes accelerate==0.20.3  # we need latest transformers for this\n",
        "# !pip install git+https://github.com/huggingface/peft.git@e536616888d51b453ed354a6f1e243fecb02ea08\n",
        "!pip -qqq install bitsandbytes accelerate\n",
        "!pip install datasets==2.10.1\n",
        "!pip install wandb\n",
        "!pip install -q -U peft\n",
        "import locale # colab workaround\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\" # colab workaround\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCmGzYg51CSH"
      },
      "source": [
        "I used 4 NVIDIA A40-48Q GPU server configured with Python 3.10 and Cuda 12.2 to run the code in this article. It ran for about eight hours. (To verify portability, I also tried running the code on Colab, and the results were good.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mu9JczX1CSH"
      },
      "source": [
        "### Load libraries \n",
        "( If get import errors, try restarting the Jupyter kernel )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oTeYW8z51CSH"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    get_peft_model_state_dict,\n",
        "    prepare_model_for_int8_training,\n",
        "    set_peft_model_state_dict,\n",
        ")\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4M9KyT0S1CSH"
      },
      "source": [
        "### 2. Load dataset 'codeparrot/xlcost-text-to-code'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ54EffO1CSI"
      },
      "source": [
        "The above extracts the dataset from Huggingface Hub and splits 10% of it into an evaluation set to check how well the model performs in training. If you want to load your own dataset, do the following:\n",
        "\n",
        "```\n",
        "train_dataset = load_dataset('json', data_files='train_set.jsonl', split='train')\n",
        "eval_dataset = load_dataset('json', data_files='validation_set.jsonl', split='train')\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "data = load_dataset(\"codeparrot/xlcost-text-to-code\", \"C++-program-level\", split=\"train\")\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each sample consists of the text \"text\" and the \"code\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = data.train_test_split(test_size=0.1)[\"train\"]\n",
        "eval_dataset = data.train_test_split(test_size=0.1)[\"test\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you want to see any sample in the dataset, just do: ``` ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "fFbeaZzf1CSJ"
      },
      "outputs": [],
      "source": [
        "print(train_dataset[3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ig7NvWN1CSJ"
      },
      "source": [
        "### 3. Load model\n",
        "I load code llama as int8 from Huggingface. Lora’s standards:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "rMnU93bY1CSJ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.64s/it]\n"
          ]
        }
      ],
      "source": [
        "base_model = \"codellama/CodeLlama-7b-hf\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-hf\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3OF-wtj1CSJ"
      },
      "source": [
        "torch_dtype=torch.float16 means to perform the calculation using a float16 representation, even if the value itself is an 8-bit integer.\n",
        "\n",
        "If you get the error \"ValueError: Tokenizer class CodeLlamaTokenizer does not exist or is not currently imported.\" make sure your Transformer version is 4.33.0 and accelerate>=0.20.3.\n",
        "\n",
        "Below is our test task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "uiyAff1a1CSJ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s> \n",
            "Use the Task below and write the Response, which is a programming code that can solve the Task.\n",
            "### Task:\n",
            "Generate a C++ program that accepts numeric input from the user and maintains a record of previous user inputs with timestamps. Ensure the program sorts the user inputs in ascending order based on the provided numeric input. Enhance the program to display timestamps along with the sorted user inputs.\n",
            "### Response:\n",
            "```cpp\n",
            "#include <iostream>\n",
            "#include <vector>\n",
            "#include <string>\n",
            "#include <fstream>\n",
            "#include <algorithm>\n",
            "#include <chrono>\n",
            "#include <ctime>\n",
            "#include <iomanip>\n",
            "#include <sstream>\n",
            "#include <iterator>\n",
            "using namespace std;\n",
            "\n",
            "int main()\n",
            "{\n",
            "\tint n;\n",
            "\tcout << \"Enter the number of inputs: \";\n",
            "\tcin >> n;\n",
            "\tvector<string> v;\n",
            "\tvector<string> v2;\n",
            "\tvector<string> v3;\n",
            "\tfor (int i = 0; i < n; i++)\n",
            "\t{\n",
            "\t\tstring s;\n",
            "\t\tcout << \"Enter the input: \";\n",
            "\t\tcin >> s;\n",
            "\t\tv.push_back(s);\n",
            "\t}\n",
            "\tfor (int i = 0; i < n; i++)\n",
            "\t{\n",
            "\t\tstringstream ss;\n",
            "\t\tss << v[i];\n",
            "\t\tint x;\n",
            "\t\tss >> x;\n",
            "\t\tstringstream s;\n",
            "\t\ts << x;\n",
            "\t\tstring s1 = s.str();\n",
            "\t\tstringstream ss1;\n",
            "\t\tss1 << i;\n",
            "\t\tstring s2 = ss1.str();\n",
            "\t\tstring s3 = s1 + \" \" + s2;\n",
            "\t\tv2.push_back(s3);\n",
            "\t}\n",
            "\tsort(v2.begin(), v2.end());\n",
            "\tfor (int i = 0; i < n; i++)\n",
            "\t{\n",
            "\t\tstringstream ss;\n",
            "\t\tss << v2[i];\n",
            "\t\tint x;\n",
            "\t\tstring s;\n",
            "\t\tss >> x;\n",
            "\t\tss >> s;\n",
            "\t\tv3.push_back(s);\n",
            "\t}\n",
            "\tofstream outfile;\n",
            "\toutfile.open(\"output.txt\");\n",
            "\tfor (int i = 0; i < n; i++)\n",
            "\t{\n",
            "\t\toutfile << v3[i] << endl;\n",
            "\t}\n",
            "\toutfile.close();\n",
            "\treturn 0;\n",
            "}\n",
            "```\n",
            "### Output:\n",
            "```cpp\n",
            "Enter the number of inputs: 5\n",
            "Enter\n"
          ]
        }
      ],
      "source": [
        "eval_prompt = \"\"\"\n",
        "Use the Task below and write the Response, which is a programming code that can solve the Task.\n",
        "### Task:\n",
        "Generate a C++ program that accepts numeric input from the user and maintains a record of previous user inputs with timestamps. Ensure the program sorts the user inputs in ascending order based on the provided numeric input. Enhance the program to display timestamps along with the sorted user inputs.\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=500, do_sample=True, top_p=0.9,temperature=0.5)[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "544rmsdU1CSJ"
      },
      "source": [
        "The output result is not perfect enough and fails the test. We have to fine-tune and check the results after fine-tuning!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is possible to pull out the number of model parameters and find out how many of them are trainable. The following function can be used to do that, at this stage, you do not need to go into details of it. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable model parameters: 262541312\n",
            "all model parameters: 6738546688\n",
            "percentage of trainable model parameters: 3.90%\n"
          ]
        }
      ],
      "source": [
        "def print_number_of_trainable_model_parameters(model):\n",
        "    trainable_model_params = 0\n",
        "    all_model_params = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_model_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_model_params += param.numel()\n",
        "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
        "\n",
        "print(print_number_of_trainable_model_parameters(model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puuOXL2R1CSJ"
      },
      "source": [
        "### 4. Tokenization\n",
        "Set some tokenization settings such as left padding since it makes [uses less memory during training](https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa)："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "c1P0mphA1CSJ"
      },
      "outputs": [],
      "source": [
        "tokenizer.add_eos_token = True\n",
        "tokenizer.pad_token_id = 0\n",
        "tokenizer.padding_side = \"left\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpo66hMo1CSJ"
      },
      "source": [
        "Set the tokenize function so that labels and input_ids are the same. This is basically [self-supervised fine-tuning](https://neptune.ai/blog/self-supervised-learning)："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "gjSerml71CSJ"
      },
      "outputs": [],
      "source": [
        "def tokenize(prompt):\n",
        "    result = tokenizer(\n",
        "        prompt,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=False,\n",
        "        return_tensors=None,\n",
        "    )\n",
        "\n",
        "    # \"self-supervised learning\" means the labels are also the inputs:\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNk_3THv1CSK"
      },
      "source": [
        "And run the tip to convert each data_point which works great:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "MvO7A-ZF1CSK"
      },
      "outputs": [],
      "source": [
        "def generate_and_tokenize_prompt(data_point):\n",
        "    full_prompt =f\"\"\"\n",
        "Use the Task below and write the Response, which is a programming code that can solve the Task.\n",
        "\n",
        "### Task:\n",
        "{data_point[\"text\"]}\n",
        "\n",
        "### Response:\n",
        "{data_point[\"code\"]}\n",
        "\"\"\"\n",
        "    return tokenize(full_prompt)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLvwo-ax1CSK"
      },
      "source": [
        "Reformat to prompt and label each sample:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "SA2BqSzW1CSK"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map:   0%|          | 0/8817 [00:00<?, ? examples/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 8817/8817 [00:10<00:00, 846.22 examples/s]\n",
            "Map: 100%|██████████| 980/980 [00:00<00:00, 1332.20 examples/s]\n"
          ]
        }
      ],
      "source": [
        "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
        "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5ByhitV1CSK"
      },
      "source": [
        "### 5. Setup Lora"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Add LoRA adapter layers/parameters to the original LLM to be trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "9q_26pz71CSK"
      },
      "outputs": [],
      "source": [
        "model.train() # put model back into training mode\n",
        "model = prepare_model_for_int8_training(model)\n",
        "\n",
        "\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0.05\n",
        "lora_r = 16\n",
        "\n",
        "config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    target_modules=[\n",
        "    \"q_proj\",\n",
        "    \"v_proj\",\n",
        "],\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable model parameters: 8388608\n",
            "all model parameters: 6746935296\n",
            "percentage of trainable model parameters: 0.12%\n"
          ]
        }
      ],
      "source": [
        "print(print_number_of_trainable_model_parameters(model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvkQ3igC1CSK"
      },
      "source": [
        "To resume from a checkpoint, set `resume_from_checkpoint` to the path of the `adapter_model.bin` from which to resume. This code will replace the connected LoRa adapter to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "TMTxR2JT1CSK"
      },
      "outputs": [],
      "source": [
        "resume_from_checkpoint = \"\" # set this to the adapter_model.bin file you want to resume from\n",
        "\n",
        "if resume_from_checkpoint:\n",
        "    if os.path.exists(resume_from_checkpoint):\n",
        "        print(f\"Restarting from {resume_from_checkpoint}\")\n",
        "        adapters_weights = torch.load(resume_from_checkpoint)\n",
        "        set_peft_model_state_dict(model, adapters_weights)\n",
        "    else:\n",
        "        print(f\"Checkpoint {resume_from_checkpoint} not found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GW1wtvbP1CSK"
      },
      "source": [
        "To view optional content of the training graph, set weights and biases:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "AC1_qWbS1CSK"
      },
      "outputs": [],
      "source": [
        "wandb_project = \"CodeLlama_finetune_CPP_2\"\n",
        "if len(wandb_project) > 0:\n",
        "    os.environ[\"WANDB_PROJECT\"] = wandb_project\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "5iv4txxu1CSK"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.device_count() > 1:\n",
        "    # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n",
        "    model.is_parallelizable = True\n",
        "    model.model_parallel = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYSsnciQ1CSK"
      },
      "source": [
        "### 6. Training Parameters\n",
        "If the GPU memory is insufficient, modify the `per_device_train_batch_size`. Ensure that the `gradient_accumulation_steps` variable is adjusted to prevent it from affecting the dynamic batching during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "uvEi1kP21CSK"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
          ]
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "batch_size = 128\n",
        "per_device_train_batch_size = 16\n",
        "gradient_accumulation_steps = batch_size // per_device_train_batch_size\n",
        "output_dir = \"CodeLlama_CPP_FineTuned_2\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "        per_device_train_batch_size=per_device_train_batch_size,\n",
        "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "        warmup_steps=100,\n",
        "        max_steps=400,\n",
        "        learning_rate=3e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=10,\n",
        "        optim=\"adamw_torch\",\n",
        "        evaluation_strategy=\"steps\", # if val_set_size > 0 else \"no\",\n",
        "        save_strategy=\"steps\",\n",
        "        eval_steps=20,\n",
        "        save_steps=20,\n",
        "        output_dir=output_dir,\n",
        "        # save_total_limit=3,\n",
        "        load_best_model_at_end=False,\n",
        "        # ddp_find_unused_parameters=False if ddp else None,\n",
        "        group_by_length=True, # group sequences of roughly the same length together to speed up training\n",
        "        report_to=\"wandb\", # if use_wandb else \"none\",\n",
        "        run_name=f\"codellama-cpp-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\", # if use_wandb else None,\n",
        "    )\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_val_dataset,\n",
        "    args=training_args,\n",
        "    data_collator=DataCollatorForSeq2Seq(\n",
        "        tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
        "    ),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aJp6Jxl1CSK"
      },
      "source": [
        "Then we perform some optimizations related to PyTorch, which are aimed at speeding up the training process without affecting accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "6ycCHZZl1CSK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "compiling the model\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "model.config.use_cache = False\n",
        "\n",
        "old_state_dict = model.state_dict\n",
        "model.state_dict = (lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())).__get__(\n",
        "    model, type(model)\n",
        ")\n",
        "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
        "    print(\"compiling the model\")\n",
        "    model = torch.compile(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "bF5oWKxK1CSL"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmedxiaorudan\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/srv/users/rudxia/Developer_MICCAI/CodeLlama/wandb/run-20240116_113709-76djvods</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/medxiaorudan/CodeLlama_finetune_CPP_2/runs/76djvods' target=\"_blank\">codellama-cpp-2024-01-16-11-36</a></strong> to <a href='https://wandb.ai/medxiaorudan/CodeLlama_finetune_CPP_2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/medxiaorudan/CodeLlama_finetune_CPP_2' target=\"_blank\">https://wandb.ai/medxiaorudan/CodeLlama_finetune_CPP_2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/medxiaorudan/CodeLlama_finetune_CPP_2/runs/76djvods' target=\"_blank\">https://wandb.ai/medxiaorudan/CodeLlama_finetune_CPP_2/runs/76djvods</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a CodeLlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [400/400 7:52:03, Epoch 5/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.590700</td>\n",
              "      <td>1.391939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.271000</td>\n",
              "      <td>1.081857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.836900</td>\n",
              "      <td>0.793175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.782000</td>\n",
              "      <td>0.755939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.683800</td>\n",
              "      <td>0.731848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.652700</td>\n",
              "      <td>0.720158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.679700</td>\n",
              "      <td>0.714182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.719500</td>\n",
              "      <td>0.705022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.714100</td>\n",
              "      <td>0.698529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.696100</td>\n",
              "      <td>0.694456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.655500</td>\n",
              "      <td>0.691649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.618400</td>\n",
              "      <td>0.686779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.645300</td>\n",
              "      <td>0.684406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.676400</td>\n",
              "      <td>0.682311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.687800</td>\n",
              "      <td>0.679458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.698400</td>\n",
              "      <td>0.676467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.644400</td>\n",
              "      <td>0.674866</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.616500</td>\n",
              "      <td>0.673838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.610700</td>\n",
              "      <td>0.672704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.662800</td>\n",
              "      <td>0.672431</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=400, training_loss=0.758864541053772, metrics={'train_runtime': 28420.8634, 'train_samples_per_second': 1.801, 'train_steps_per_second': 0.014, 'total_flos': 7.593082346579558e+17, 'train_loss': 0.758864541053772, 'epoch': 5.8})"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1dRQLMT1CSU"
      },
      "source": [
        "### Load the final checkpoint.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "sRdVgDTg1CSU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.52s/it]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
        "\n",
        "base_model = \"codellama/CodeLlama-7b-hf\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-hf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76UlHzhy1CSU"
      },
      "source": [
        "To load a fine-tuned Lora/Qlora adapter, use `PeftModel.from_pretrained`. The `output_dir` should contain `adapter_config.json` and `adapter_model.bin`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "rQrCR0os1CSU"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "model = PeftModel.from_pretrained(model, \"/srv/users/rudxia/Developer_MICCAI/CodeLlama/CodeLlama_CPP_FineTuned_2/checkpoint-400/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The number of trainable parameters will be `0` due to `is_trainable=False` setting:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable model parameters: 0\n",
            "all model parameters: 6746935296\n",
            "percentage of trainable model parameters: 0.00%\n"
          ]
        }
      ],
      "source": [
        "print(print_number_of_trainable_model_parameters(model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roqy_WRi1CSU"
      },
      "source": [
        "Attempt with the same prompts as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GrCxouNp1CSU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s> \n",
            "Use the Task below and write the Response, which is a programming code that can solve the Task.\n",
            "### Task:\n",
            "Generate a C++ program that accepts numeric input from the user and maintains a record of previous user inputs with timestamps. Ensure the program sorts the user inputs in ascending order based on the provided numeric input. Enhance the program to display timestamps along with the sorted user inputs.\n",
            "### Response:\n",
            "```\n",
            "#include <iostream>\n",
            "#include <string>\n",
            "#include <vector>\n",
            "#include <algorithm>\n",
            "#include <ctime>\n",
            "#include <iomanip>\n",
            "\n",
            "using namespace std;\n",
            "\n",
            "int main()\n",
            "{\n",
            "    vector<string> inputs;\n",
            "    string input;\n",
            "    string time;\n",
            "\n",
            "    cout << \"Enter input: \";\n",
            "    getline(cin, input);\n",
            "\n",
            "    while (input != \"exit\")\n",
            "    {\n",
            "        time = to_string(time_t(time(0)));\n",
            "        inputs.push_back(input + \" \" + time);\n",
            "        cout << \"Enter input: \";\n",
            "        getline(cin, input);\n",
            "    }\n",
            "\n",
            "    sort(inputs.begin(), inputs.end());\n",
            "\n",
            "    for (int i = 0; i < inputs.size(); i++)\n",
            "    {\n",
            "        cout << left << setw(35) << inputs[i] << endl;\n",
            "    }\n",
            "}\n",
            "```\n",
            "\n",
            "### Explanation:\n",
            "\n",
            "##### Step 1:\n",
            "\n",
            "Include all the necessary header files.\n",
            "\n",
            "##### Step 2:\n",
            "\n",
            "Declare a vector to store the inputs and the string to store the time.\n",
            "\n",
            "##### Step 3:\n",
            "\n",
            "Accept the input from the user and store it in the vector.\n",
            "\n",
            "##### Step 4:\n",
            "\n",
            "Use the sort function to sort the vector in ascending order.\n",
            "\n",
            "##### Step 5:\n",
            "\n",
            "Use the for loop to display the sorted vector.\n",
            "\n",
            "### Output:\n",
            "\n",
            "![Output](https://github.com/Rishika-dagar/C-CPP-Programming-Language-Exercises/blob/main/C%2B%2B%20Programming%20Language%20Exercises/Exercise%2011/Images/Output.png?raw=true)\n",
            "\n",
            "### Author: [Rishika Dagar](https://github.com/Rishika-dagar)\n",
            "\n",
            "### Date: 2022-03-18\n",
            "\n",
            "### Programming Language Used: [C++](https://www.cplusplus.com/)\n",
            "\n",
            "### Question: [Exercise 1\n"
          ]
        }
      ],
      "source": [
        "eval_prompt = \"\"\"\n",
        "Use the Task below and write the Response, which is a programming code that can solve the Task.\n",
        "### Task:\n",
        "Generate a C++ program that accepts numeric input from the user and maintains a record of previous user inputs with timestamps. Ensure the program sorts the user inputs in ascending order based on the provided numeric input. Enhance the program to display timestamps along with the sorted user inputs.\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=500, pad_token_id=tokenizer.eos_token_id,do_sample=True, top_p=0.9,temperature=0.5)[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6IcTOCq1CSU"
      },
      "source": [
        "From the running results, it can be seen that fine-tuning is effective! It can successfully pass the compilation tests. You can also convert this adapter to the Llama.cpp model for running locally.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
