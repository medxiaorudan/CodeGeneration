{"cells":[{"cell_type":"markdown","metadata":{"id":"qMdW0KaMacQf"},"source":["### Open Source LLM\n","\n","We can combine LLamaCPP or [Ollama integrated](https://ollama.ai) to use [Code LLaMA](https://about.fb.com/news/2023/08/code-llama-ai-for-coding/) /blog/run-code-llama-locally）.\n","\n","Note：Please update `llama-cpp-python` , so that the `gguf` [format](https://github.com/abetlen/llama-cpp-python/pull/633) can be used here.\n","\n","````\n","CMAKE_ARGS =“-DLLAMA_METAL = on”FORCE_CMAKE = 1 /Users/rlm/miniforge3/envs/llama2/bin/pip install -U llama-cpp-python --no-cache-dir\n","````\n","\n","Check the lastest code-llama model [here](https://huggingface.co/TheBloke/CodeLlama-13B-Instruct-GGUF/tree/main)."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36579,"status":"ok","timestamp":1694145717564,"user":{"displayName":"MENGYANG LU","userId":"11275008728594910650"},"user_tz":-480},"id":"JK2AXJuQ8zN-","outputId":"bf913e92-674f-4cb3-84cf-516c291cce95"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting llama-cpp-python\n","  Downloading llama_cpp_python-0.2.28.tar.gz (9.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n","\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /srv/users/rudxia/anaconda3/envs/GPT4/lib/python3.8/site-packages (from llama-cpp-python) (4.9.0)\n","Requirement already satisfied: numpy>=1.20.0 in /srv/users/rudxia/anaconda3/envs/GPT4/lib/python3.8/site-packages (from llama-cpp-python) (1.24.4)\n","Collecting diskcache>=5.6.1 (from llama-cpp-python)\n","  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n","Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n","  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.28-cp38-cp38-manylinux_2_31_x86_64.whl size=2113205 sha256=b4ce51d3f00900add3524f1b5ce69bc7602159f986c6ca4e7bc86ca7090370cb\n","  Stored in directory: /srv/users/rudxia/.cache/pip/wheels/12/12/f8/9e08a6fefe1455c9f86713e5c834fa8cfc117c64ca82c53c25\n","Successfully built llama-cpp-python\n","Installing collected packages: diskcache, llama-cpp-python\n","Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.28\n"]}],"source":["!pip install -U llama-cpp-python\n","!pip install openai tiktoken chromadb langchain"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1694145633209,"user":{"displayName":"MENGYANG LU","userId":"11275008728594910650"},"user_tz":-480},"id":"I69RB2W_acQf"},"outputs":[],"source":["from langchain import PromptTemplate\n","from langchain.llms import LlamaCpp\n","from langchain.chains import LLMChain\n","from langchain.callbacks.manager import CallbackManager\n","from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":76376,"status":"ok","timestamp":1694145803915,"user":{"displayName":"MENGYANG LU","userId":"11275008728594910650"},"user_tz":-480},"id":"_0SIH6eqacQf","outputId":"d22c95a4-9de6-49ef-b659-fe49dd55e61c","scrolled":true},"outputs":[{"name":"stderr","output_type":"stream","text":["llama_model_loader: loaded meta data with 20 key-value pairs and 363 tensors from /srv/users/rudxia/Developer_MICCAI/CodeLlama/model/codellama-13b-instruct.Q3_K_M.gguf (version GGUF V2)\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.name str              = codellama_codellama-13b-instruct-hf\n","llama_model_loader: - kv   2:                       llama.context_length u32              = 16384\n","llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n","llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n","llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n","llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n","llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n","llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n","llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n","llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n","llama_model_loader: - kv  11:                          general.file_type u32              = 12\n","llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n","llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32016]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n","llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32016]   = [0.000000, 0.000000, 0.000000, 0.0000...\n","llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32016]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n","llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n","llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n","llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n","llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n","llama_model_loader: - type  f32:   81 tensors\n","llama_model_loader: - type q3_K:  161 tensors\n","llama_model_loader: - type q4_K:  116 tensors\n","llama_model_loader: - type q5_K:    4 tensors\n","llama_model_loader: - type q6_K:    1 tensors\n","llm_load_vocab: mismatch in special tokens definition ( 264/32016 vs 259/32016 ).\n","llm_load_print_meta: format           = GGUF V2\n","llm_load_print_meta: arch             = llama\n","llm_load_print_meta: vocab type       = SPM\n","llm_load_print_meta: n_vocab          = 32016\n","llm_load_print_meta: n_merges         = 0\n","llm_load_print_meta: n_ctx_train      = 16384\n","llm_load_print_meta: n_embd           = 5120\n","llm_load_print_meta: n_head           = 40\n","llm_load_print_meta: n_head_kv        = 40\n","llm_load_print_meta: n_layer          = 40\n","llm_load_print_meta: n_rot            = 128\n","llm_load_print_meta: n_embd_head_k    = 128\n","llm_load_print_meta: n_embd_head_v    = 128\n","llm_load_print_meta: n_gqa            = 1\n","llm_load_print_meta: n_embd_k_gqa     = 5120\n","llm_load_print_meta: n_embd_v_gqa     = 5120\n","llm_load_print_meta: f_norm_eps       = 0.0e+00\n","llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n","llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n","llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n","llm_load_print_meta: n_ff             = 13824\n","llm_load_print_meta: n_expert         = 0\n","llm_load_print_meta: n_expert_used    = 0\n","llm_load_print_meta: rope scaling     = linear\n","llm_load_print_meta: freq_base_train  = 1000000.0\n","llm_load_print_meta: freq_scale_train = 1\n","llm_load_print_meta: n_yarn_orig_ctx  = 16384\n","llm_load_print_meta: rope_finetuned   = unknown\n","llm_load_print_meta: model type       = 13B\n","llm_load_print_meta: model ftype      = Q3_K - Medium\n","llm_load_print_meta: model params     = 13.02 B\n","llm_load_print_meta: model size       = 5.90 GiB (3.89 BPW) \n","llm_load_print_meta: general.name     = codellama_codellama-13b-instruct-hf\n","llm_load_print_meta: BOS token        = 1 '<s>'\n","llm_load_print_meta: EOS token        = 2 '</s>'\n","llm_load_print_meta: UNK token        = 0 '<unk>'\n","llm_load_print_meta: LF token         = 13 '<0x0A>'\n","llm_load_tensors: ggml ctx size =    0.14 MiB\n","llm_load_tensors: offloading 1 repeating layers to GPU\n","llm_load_tensors: offloaded 1/41 layers to GPU\n","llm_load_tensors:        CPU buffer size =  6043.55 MiB\n","...................................................................................................\n","llama_new_context_with_model: n_ctx      = 100000\n","llama_new_context_with_model: freq_base  = 10000.0\n","llama_new_context_with_model: freq_scale = 1\n","llama_kv_cache_init:        CPU KV buffer size = 78125.00 MiB\n","llama_new_context_with_model: KV self size  = 78125.00 MiB, K (f16): 39062.50 MiB, V (f16): 39062.50 MiB\n","llama_new_context_with_model: graph splits (measure): 1\n","llama_new_context_with_model:        CPU compute buffer size =  8037.82 MiB\n","AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"]}],"source":["callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n","llm = LlamaCpp(\n","    model_path=\"/srv/users/rudxia/Developer_MICCAI/CodeLlama/model/codellama-13b-instruct.Q3_K_M.gguf\",\n","    n_ctx=100000,\n","    n_gpu_layers=1,\n","    n_batch=512,\n","    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n","    callback_manager=callback_manager,\n","    verbose=True,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## **Prompt after formatting**"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\u001b[1m> Entering new LLMChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGenerate a C++ program that accepts numeric input from the user and maintains a record of previous user inputs with timestamps. Ensure the program sorts the user inputs in ascending order based on the provided numeric input. Enhance the program to display timestamps along with the sorted user inputs.\u001b[0m\n","\n","\n","#include <iostream>\n","#include <iomanip>\n","using namespace std;\n","int main() {\n","\tcout<<\"Please enter the number:\";\n","\tdouble num, temp = 0;\n","\tcin>>num;\n","\tif (num % 2 == 0) {\n","\t\ttemp = num / 2;\n","\t} else if ((num + 1) % 2 == 0)) {\n","\t\ttemp = ((num + 1))) / 2;\n","\t}\n","\tcout<<\"The number is: \" << temp << endl;\n","\n","\n","\n","\n","\n","return 0;\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["\n","llama_print_timings:        load time =    2303.34 ms\n","llama_print_timings:      sample time =      41.32 ms /   154 runs   (    0.27 ms per token,  3727.01 tokens per second)\n","llama_print_timings: prompt eval time =    2301.16 ms /    64 tokens (   35.96 ms per token,    27.81 tokens per second)\n","llama_print_timings:        eval time =   21380.00 ms /   153 runs   (  139.74 ms per token,     7.16 tokens per second)\n","llama_print_timings:       total time =   24370.36 ms\n"]},{"data":{"text/plain":["'\\n\\n#include <iostream>\\n#include <iomanip>\\nusing namespace std;\\nint main() {\\n\\tcout<<\"Please enter the number:\";\\n\\tdouble num, temp = 0;\\n\\tcin>>num;\\n\\tif (num % 2 == 0) {\\n\\t\\ttemp = num / 2;\\n\\t} else if ((num + 1) % 2 == 0)) {\\n\\t\\ttemp = ((num + 1))) / 2;\\n\\t}\\n\\tcout<<\"The number is: \" << temp << endl;\\n\\n\\n\\n\\n\\nreturn 0;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["question1=\"Generate a C++ program that accepts numeric input from the user and maintains a record of previous user inputs with timestamps. Ensure the program sorts the user inputs in ascending order based on the provided numeric input. Enhance the program to display timestamps along with the sorted user inputs.\"\n","prompt=PromptTemplate.from_template(question1)\n","\n","default_chain=LLMChain(llm=llm, prompt=prompt, verbose=True)\n","default_chain.run(prompt=question1)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\u001b[1m> Entering new LLMChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGenerate a C++ program that accepts numeric input from the user and maintains a record of previous user inputs with timestamps. Include error handling to handle non-numeric inputs and provide a message to the user when exiting the program. Include the C++ standard library algorithm header for std::sort to ensure the program can sort the user inputs in ascending order based on the provided numeric input. Use a suitable data structure to efficiently store and manage the user input and timestamps records. Implement C++ standard library vector header. Include the C++ standard library ctime header for handling timestamps. Implement a function to display the sorted user input records with timestamps.\u001b[0m\n"," Implement a function to exit the program from within the program.\n","### Sample Output ###\n","\n","  -----------------------------------------------\n","    |                                                                |\n","      --------------------------\n","\n","\n","\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["\n","llama_print_timings:        load time =    5543.33 ms\n","llama_print_timings:      sample time =      14.25 ms /    46 runs   (    0.31 ms per token,  3227.39 tokens per second)\n","llama_print_timings: prompt eval time =    5542.52 ms /   144 tokens (   38.49 ms per token,    25.98 tokens per second)\n","llama_print_timings:        eval time =    7069.67 ms /    45 runs   (  157.10 ms per token,     6.37 tokens per second)\n","llama_print_timings:       total time =   12815.91 ms /   189 tokens\n"]},{"data":{"text/plain":["' Implement a function to exit the program from within the program.\\n### Sample Output ###\\n\\n  -----------------------------------------------\\n    |                                                                |\\n      --------------------------\\n\\n\\n\\n'"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["question2=\"Generate a C++ program that accepts numeric input from the user and maintains a record of previous user inputs with timestamps. \\\n","Include error handling to handle non-numeric inputs and provide a message to the user when exiting the program. \\\n","Include the C++ standard library algorithm header for std::sort to ensure the program can sort the user inputs in ascending order based on the provided numeric input. \\\n","Use a suitable data structure to efficiently store and manage the user input and timestamps records. \\\n","Implement C++ standard library vector header. Include the C++ standard library ctime header for handling timestamps. \\\n","Implement a function to display the sorted user input records with timestamps.\"\n","\n","prompt=PromptTemplate.from_template(question2)\n","\n","default_chain=LLMChain(llm=llm, prompt=prompt, verbose=True)\n","default_chain.run(prompt=question2)"]},{"cell_type":"markdown","metadata":{},"source":["## **Prompt directly**"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\\"]},{"name":"stderr","output_type":"stream","text":["Llama.generate: prefix-match hit\n"]},{"name":"stdout","output_type":"stream","text":["begin{code}\n","#include <iostream>\n","using namespace std;\n","int main() {\n","    cout << \"Enter an integer: \";\n","    int userInput = 0, tempIntVar = 0;\n","    cin >> userInput;\n","    if (userInput > 1) {\n","        cout << \"\\n\";\n","        for (int counter = 2; counter <= userInput; ++counter) {\n","            cout << counter << \" \";\n","            if (counter % 3 == 0)) {\n","                cout << endl;\n","            } else if ((counter >= 5) && (counter % 5 == 0))))))) {\n","                        cout << \"\\n\";\n","                        for (int counter = 2; counter <= userInput; ++counter) {\n","                                    cout << counter << \" \";\n","                                    if (counter % 3 == 0)) {\n","                                                            cout << endl;\n","                                                        } else if (((counter >= 5) && (counter % 5 == 0))) {\n","                                                                                                                cout << \"\\n\";\n","                                                                                                                    for (int counter ="]},{"name":"stderr","output_type":"stream","text":["\n","llama_print_timings:        load time =    5911.12 ms\n","llama_print_timings:      sample time =      69.68 ms /   256 runs   (    0.27 ms per token,  3674.04 tokens per second)\n","llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n","llama_print_timings:        eval time =   37557.97 ms /   256 runs   (  146.71 ms per token,     6.82 tokens per second)\n","llama_print_timings:       total time =   38747.29 ms /   257 tokens\n"]},{"data":{"text/plain":["'\\\\begin{code}\\n#include <iostream>\\nusing namespace std;\\nint main() {\\n    cout << \"Enter an integer: \";\\n    int userInput = 0, tempIntVar = 0;\\n    cin >> userInput;\\n    if (userInput > 1) {\\n        cout << \"\\\\n\";\\n        for (int counter = 2; counter <= userInput; ++counter) {\\n            cout << counter << \" \";\\n            if (counter % 3 == 0)) {\\n                cout << endl;\\n            } else if ((counter >= 5) && (counter % 5 == 0))))))) {\\n                        cout << \"\\\\n\";\\n                        for (int counter = 2; counter <= userInput; ++counter) {\\n                                    cout << counter << \" \";\\n                                    if (counter % 3 == 0)) {\\n                                                            cout << endl;\\n                                                        } else if (((counter >= 5) && (counter % 5 == 0))) {\\n                                                                                                                cout << \"\\\\n\";\\n                                                                                                                    for (int counter ='"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["prompt1 = \"\"\"\n","Question: Generate a C++ program that accepts numeric input from the user and maintains a record of previous user inputs with timestamps. Ensure the program sorts the user inputs in ascending order based on the provided numeric input. Enhance the program to display timestamps along with the sorted user inputs.\n","\"\"\"\n","llm(prompt1)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Llama.generate: prefix-match hit\n"]},{"name":"stdout","output_type":"stream","text":["Comment: Welcome to Stackoverflow. Please read [How do I ask a good question?](https://stackoverflow.com/help/how-to-ask) and update your question accordingly.\n","Answer: You have many mistakes in your code, but I will not write all of them here because it would be very long. But you can solve your problem by looking at the corrected version of your program below.\n","\\begin{code}\n","#include <iostream>\n","using namespace std;\n","//function to sort user inputs based on their numeric value\n","void sortInputs(string &userInput1, string &userInput2, string &userInput3)){\n","  //define a variable called \"sortedUserInputs\" as an array of three strings.\n","   sortedUserInputs[0] = userInput1;\n","    sortedUserInputs[1] = userInput2;\n","     sortedUserInputs[2] = userInput3;\n","       //define a variable called \"tempString\" as a string.\n","            tempString = \"\";\n","\n","                   //define a variable called \"userInput1\", \"userInput2\", and \"userInput3\" as three strings each that will store the numeric input provided by the"]},{"name":"stderr","output_type":"stream","text":["\n","llama_print_timings:        load time =    5543.33 ms\n","llama_print_timings:      sample time =      73.37 ms /   256 runs   (    0.29 ms per token,  3489.35 tokens per second)\n","llama_print_timings: prompt eval time =    4788.57 ms /   148 tokens (   32.36 ms per token,    30.91 tokens per second)\n","llama_print_timings:        eval time =   38825.58 ms /   255 runs   (  152.26 ms per token,     6.57 tokens per second)\n","llama_print_timings:       total time =   44794.33 ms /   403 tokens\n"]},{"data":{"text/plain":["'Comment: Welcome to Stackoverflow. Please read [How do I ask a good question?](https://stackoverflow.com/help/how-to-ask) and update your question accordingly.\\nAnswer: You have many mistakes in your code, but I will not write all of them here because it would be very long. But you can solve your problem by looking at the corrected version of your program below.\\n\\\\begin{code}\\n#include <iostream>\\nusing namespace std;\\n//function to sort user inputs based on their numeric value\\nvoid sortInputs(string &userInput1, string &userInput2, string &userInput3)){\\n  //define a variable called \"sortedUserInputs\" as an array of three strings.\\n   sortedUserInputs[0] = userInput1;\\n    sortedUserInputs[1] = userInput2;\\n     sortedUserInputs[2] = userInput3;\\n       //define a variable called \"tempString\" as a string.\\n            tempString = \"\";\\n\\n                   //define a variable called \"userInput1\", \"userInput2\", and \"userInput3\" as three strings each that will store the numeric input provided by the'"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["prompt2 = \"\"\"\n","Question: Generate a C++ program that accepts numeric input from the user and maintains a record of previous user inputs with timestamps. \\\n","Include error handling to handle non-numeric inputs and provide a message to the user when exiting the program. \\\n","Include the C++ standard library algorithm header for std::sort to ensure the program can sort the user inputs in ascending order based on the provided numeric input. \\\n","Use a suitable data structure to efficiently store and manage the user input and timestamps records. \\\n","Implement C++ standard library vector header. Include the C++ standard library ctime header for handling timestamps. \\\n","Implement a function to display the sorted user input records with timestamps.\n","\"\"\"\n","llm(prompt2)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["llama_model_loader: loaded meta data with 20 key-value pairs and 363 tensors from /srv/users/rudxia/Developer_MICCAI/CodeLlama/model/codellama-13b-instruct.Q3_K_M.gguf (version GGUF V2)\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.name str              = codellama_codellama-13b-instruct-hf\n","llama_model_loader: - kv   2:                       llama.context_length u32              = 16384\n","llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n","llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n","llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n","llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n","llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n","llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n","llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n","llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n","llama_model_loader: - kv  11:                          general.file_type u32              = 12\n","llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n","llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32016]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n","llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32016]   = [0.000000, 0.000000, 0.000000, 0.0000...\n","llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32016]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n","llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n","llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n","llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n","llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n","llama_model_loader: - type  f32:   81 tensors\n","llama_model_loader: - type q3_K:  161 tensors\n","llama_model_loader: - type q4_K:  116 tensors\n","llama_model_loader: - type q5_K:    4 tensors\n","llama_model_loader: - type q6_K:    1 tensors\n","llm_load_vocab: mismatch in special tokens definition ( 264/32016 vs 259/32016 ).\n","llm_load_print_meta: format           = GGUF V2\n","llm_load_print_meta: arch             = llama\n","llm_load_print_meta: vocab type       = SPM\n","llm_load_print_meta: n_vocab          = 32016\n","llm_load_print_meta: n_merges         = 0\n","llm_load_print_meta: n_ctx_train      = 16384\n","llm_load_print_meta: n_embd           = 5120\n","llm_load_print_meta: n_head           = 40\n","llm_load_print_meta: n_head_kv        = 40\n","llm_load_print_meta: n_layer          = 40\n","llm_load_print_meta: n_rot            = 128\n","llm_load_print_meta: n_embd_head_k    = 128\n","llm_load_print_meta: n_embd_head_v    = 128\n","llm_load_print_meta: n_gqa            = 1\n","llm_load_print_meta: n_embd_k_gqa     = 5120\n","llm_load_print_meta: n_embd_v_gqa     = 5120\n","llm_load_print_meta: f_norm_eps       = 0.0e+00\n","llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n","llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n","llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n","llm_load_print_meta: n_ff             = 13824\n","llm_load_print_meta: n_expert         = 0\n","llm_load_print_meta: n_expert_used    = 0\n","llm_load_print_meta: rope scaling     = linear\n","llm_load_print_meta: freq_base_train  = 1000000.0\n","llm_load_print_meta: freq_scale_train = 1\n","llm_load_print_meta: n_yarn_orig_ctx  = 16384\n","llm_load_print_meta: rope_finetuned   = unknown\n","llm_load_print_meta: model type       = 13B\n","llm_load_print_meta: model ftype      = Q3_K - Medium\n","llm_load_print_meta: model params     = 13.02 B\n","llm_load_print_meta: model size       = 5.90 GiB (3.89 BPW) \n","llm_load_print_meta: general.name     = codellama_codellama-13b-instruct-hf\n","llm_load_print_meta: BOS token        = 1 '<s>'\n","llm_load_print_meta: EOS token        = 2 '</s>'\n","llm_load_print_meta: UNK token        = 0 '<unk>'\n","llm_load_print_meta: LF token         = 13 '<0x0A>'\n","llm_load_tensors: ggml ctx size =    0.14 MiB\n","llm_load_tensors: offloading 1 repeating layers to GPU\n","llm_load_tensors: offloaded 1/41 layers to GPU\n","llm_load_tensors:        CPU buffer size =  6043.55 MiB\n","...................................................................................................\n","llama_new_context_with_model: n_ctx      = 100000\n","llama_new_context_with_model: freq_base  = 10000.0\n","llama_new_context_with_model: freq_scale = 1\n","llama_kv_cache_init:        CPU KV buffer size = 78125.00 MiB\n","llama_new_context_with_model: KV self size  = 78125.00 MiB, K (f16): 39062.50 MiB, V (f16): 39062.50 MiB\n","llama_new_context_with_model: graph splits (measure): 1\n","llama_new_context_with_model:        CPU compute buffer size =  8037.82 MiB\n","AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","\u001b[1m> Entering new LLMChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3m#include <iostream>\n","#include <vector>\n","#include <algorithm>\n","#include <ctime>\n","Generate a C++ program that accepts numeric input from the user and maintains a record of previous user inputs with timestamps. Include error handling to handle non-numeric inputs and provide a message to the user when exiting the program. Include the C++ standard library algorithm header for std::sort to ensure the program can sort the user inputs in ascending order based on the provided numeric input. Use a suitable data structure to efficiently store and manage the user input and timestamps records. Implement C++ standard library vector header. Include the C++ standard library ctime header for handling timestamps. Implement a function to display the sorted user input records with timestamps.\u001b[0m\n","\n","#include <iostream>\n","int main() {\n","std::cout << \"Hello World!\" << std::endl;\n","return 0;\n","}\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["\n","llama_print_timings:        load time =    5605.11 ms\n","llama_print_timings:      sample time =       8.73 ms /    33 runs   (    0.26 ms per token,  3781.37 tokens per second)\n","llama_print_timings: prompt eval time =    5602.53 ms /   169 tokens (   33.15 ms per token,    30.16 tokens per second)\n","llama_print_timings:        eval time =    5086.33 ms /    32 runs   (  158.95 ms per token,     6.29 tokens per second)\n","llama_print_timings:       total time =   10830.45 ms /   201 tokens\n"]},{"data":{"text/plain":["'\\n#include <iostream>\\nint main() {\\nstd::cout << \"Hello World!\" << std::endl;\\nreturn 0;\\n}'"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["from langchain import PromptTemplate\n","from langchain.llms import LlamaCpp\n","from langchain.chains import LLMChain\n","from langchain.callbacks.manager import CallbackManager\n","from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n","\n","callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n","llm = LlamaCpp(\n","    model_path=\"/srv/users/rudxia/Developer_MICCAI/CodeLlama/model/codellama-13b-instruct.Q3_K_M.gguf\",\n","    n_ctx=100000,\n","    n_gpu_layers=1,\n","    n_batch=512,\n","    f16_kv=True,  # MUST set to True, otherwise you will run into problems after a couple of calls\n","    callback_manager=callback_manager,\n","    verbose=True,\n",")\n","\n","question2 = \"Generate a C++ program that accepts numeric input from the user and maintains a record of previous user inputs with timestamps. \\\n","Include error handling to handle non-numeric inputs and provide a message to the user when exiting the program. \\\n","Include the C++ standard library algorithm header for std::sort to ensure the program can sort the user inputs in ascending order based on the provided numeric input. \\\n","Use a suitable data structure to efficiently store and manage the user input and timestamps records. \\\n","Implement C++ standard library vector header. Include the C++ standard library ctime header for handling timestamps. \\\n","Implement a function to display the sorted user input records with timestamps.\"\n","\n","# Ensure that the prompt contains placeholders for the library includes\n","prompt_with_includes = \"#include <iostream>\\n#include <vector>\\n#include <algorithm>\\n#include <ctime>\\n\" + question2\n","\n","prompt_template = PromptTemplate.from_template(prompt_with_includes)\n","\n","default_chain = LLMChain(llm=llm, prompt=prompt_template, verbose=True)\n","default_chain.run(prompt=prompt_with_includes)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[{"file_id":"https://github.com/langchain-ai/langchain/blob/master/docs/extras/use_cases/code_understanding.ipynb","timestamp":1694086367701}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":0}
