{"cells":[{"cell_type":"markdown","metadata":{"id":"qMdW0KaMacQf"},"source":["### Open Source LLM\n","\n","We can combine LLamaCPP or [Ollama integrated](https://ollama.ai) to use [Code LLaMA](https://about.fb.com/news/2023/08/code-llama-ai-for-coding/) /blog/run-code-llama-locally）.\n","\n","Note：Please update `llama-cpp-python` , so that the `gguf` [format](https://github.com/abetlen/llama-cpp-python/pull/633) can be used here.\n","\n","````\n","CMAKE_ARGS =“-DLLAMA_METAL = on”FORCE_CMAKE = 1 /Users/rlm/miniforge3/envs/llama2/bin/pip install -U llama-cpp-python --no-cache-dir\n","````\n","\n","Check the lastest code-llama model [here](https://huggingface.co/TheBloke/CodeLlama-13B-Instruct-GGUF/tree/main)."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36579,"status":"ok","timestamp":1694145717564,"user":{"displayName":"MENGYANG LU","userId":"11275008728594910650"},"user_tz":-480},"id":"JK2AXJuQ8zN-","outputId":"bf913e92-674f-4cb3-84cf-516c291cce95"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting llama-cpp-python\n","  Downloading llama_cpp_python-0.2.28.tar.gz (9.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n","\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /srv/users/rudxia/anaconda3/envs/GPT4/lib/python3.8/site-packages (from llama-cpp-python) (4.9.0)\n","Requirement already satisfied: numpy>=1.20.0 in /srv/users/rudxia/anaconda3/envs/GPT4/lib/python3.8/site-packages (from llama-cpp-python) (1.24.4)\n","Collecting diskcache>=5.6.1 (from llama-cpp-python)\n","  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n","Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n","  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.28-cp38-cp38-manylinux_2_31_x86_64.whl size=2113205 sha256=b4ce51d3f00900add3524f1b5ce69bc7602159f986c6ca4e7bc86ca7090370cb\n","  Stored in directory: /srv/users/rudxia/.cache/pip/wheels/12/12/f8/9e08a6fefe1455c9f86713e5c834fa8cfc117c64ca82c53c25\n","Successfully built llama-cpp-python\n","Installing collected packages: diskcache, llama-cpp-python\n","Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.28\n"]}],"source":["!pip install -U llama-cpp-python\n","!pip install openai tiktoken chromadb langchain"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1694145633209,"user":{"displayName":"MENGYANG LU","userId":"11275008728594910650"},"user_tz":-480},"id":"I69RB2W_acQf"},"outputs":[],"source":["from langchain import PromptTemplate\n","from langchain.llms import LlamaCpp\n","from langchain.chains import LLMChain\n","from langchain.callbacks.manager import CallbackManager\n","from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":76376,"status":"ok","timestamp":1694145803915,"user":{"displayName":"MENGYANG LU","userId":"11275008728594910650"},"user_tz":-480},"id":"_0SIH6eqacQf","outputId":"d22c95a4-9de6-49ef-b659-fe49dd55e61c","scrolled":true},"outputs":[{"name":"stderr","output_type":"stream","text":["llama_model_loader: loaded meta data with 20 key-value pairs and 363 tensors from /srv/users/rudxia/Developer_MICCAI/CodeLlama/model/codellama-13b-instruct.Q3_K_M.gguf (version GGUF V2)\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.name str              = codellama_codellama-13b-instruct-hf\n","llama_model_loader: - kv   2:                       llama.context_length u32              = 16384\n","llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n","llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n","llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n","llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n","llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n","llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n","llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n","llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n","llama_model_loader: - kv  11:                          general.file_type u32              = 12\n","llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n","llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32016]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n","llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32016]   = [0.000000, 0.000000, 0.000000, 0.0000...\n","llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32016]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n","llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n","llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n","llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n","llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n","llama_model_loader: - type  f32:   81 tensors\n","llama_model_loader: - type q3_K:  161 tensors\n","llama_model_loader: - type q4_K:  116 tensors\n","llama_model_loader: - type q5_K:    4 tensors\n","llama_model_loader: - type q6_K:    1 tensors\n","llm_load_vocab: mismatch in special tokens definition ( 264/32016 vs 259/32016 ).\n","llm_load_print_meta: format           = GGUF V2\n","llm_load_print_meta: arch             = llama\n","llm_load_print_meta: vocab type       = SPM\n","llm_load_print_meta: n_vocab          = 32016\n","llm_load_print_meta: n_merges         = 0\n","llm_load_print_meta: n_ctx_train      = 16384\n","llm_load_print_meta: n_embd           = 5120\n","llm_load_print_meta: n_head           = 40\n","llm_load_print_meta: n_head_kv        = 40\n","llm_load_print_meta: n_layer          = 40\n","llm_load_print_meta: n_rot            = 128\n","llm_load_print_meta: n_embd_head_k    = 128\n","llm_load_print_meta: n_embd_head_v    = 128\n","llm_load_print_meta: n_gqa            = 1\n","llm_load_print_meta: n_embd_k_gqa     = 5120\n","llm_load_print_meta: n_embd_v_gqa     = 5120\n","llm_load_print_meta: f_norm_eps       = 0.0e+00\n","llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n","llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n","llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n","llm_load_print_meta: n_ff             = 13824\n","llm_load_print_meta: n_expert         = 0\n","llm_load_print_meta: n_expert_used    = 0\n","llm_load_print_meta: rope scaling     = linear\n","llm_load_print_meta: freq_base_train  = 1000000.0\n","llm_load_print_meta: freq_scale_train = 1\n","llm_load_print_meta: n_yarn_orig_ctx  = 16384\n","llm_load_print_meta: rope_finetuned   = unknown\n","llm_load_print_meta: model type       = 13B\n","llm_load_print_meta: model ftype      = Q3_K - Medium\n","llm_load_print_meta: model params     = 13.02 B\n","llm_load_print_meta: model size       = 5.90 GiB (3.89 BPW) \n","llm_load_print_meta: general.name     = codellama_codellama-13b-instruct-hf\n","llm_load_print_meta: BOS token        = 1 '<s>'\n","llm_load_print_meta: EOS token        = 2 '</s>'\n","llm_load_print_meta: UNK token        = 0 '<unk>'\n","llm_load_print_meta: LF token         = 13 '<0x0A>'\n","llm_load_tensors: ggml ctx size =    0.14 MiB\n","llm_load_tensors: offloading 1 repeating layers to GPU\n","llm_load_tensors: offloaded 1/41 layers to GPU\n","llm_load_tensors:        CPU buffer size =  6043.55 MiB\n","...................................................................................................\n","llama_new_context_with_model: n_ctx      = 100000\n","llama_new_context_with_model: freq_base  = 10000.0\n","llama_new_context_with_model: freq_scale = 1\n","llama_kv_cache_init:        CPU KV buffer size = 78125.00 MiB\n","llama_new_context_with_model: KV self size  = 78125.00 MiB, K (f16): 39062.50 MiB, V (f16): 39062.50 MiB\n","llama_new_context_with_model: graph splits (measure): 1\n","llama_new_context_with_model:        CPU compute buffer size =  8037.82 MiB\n","AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"]}],"source":["callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n","llm = LlamaCpp(\n","    model_path=\"/srv/users/rudxia/Developer_MICCAI/CodeLlama/model/codellama-13b-instruct.Q3_K_M.gguf\",\n","    n_ctx=100000,\n","    n_gpu_layers=1,\n","    n_batch=512,\n","    f16_kv=False,  # MUST set to True, otherwise you will run into problem after a couple of calls\n","    callback_manager=callback_manager,\n","    verbose=True,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## **Prompt after formatting**"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\u001b[1m> Entering new LLMChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGenerate a C++ program that accepts numeric input from the user and maintains a record of previous user inputs with timestamps. Ensure the program sorts the user inputs in ascending order based on the provided numeric input. Enhance the program to display timestamps along with the sorted user inputs.\u001b[0m\n","\n","\n","#include <iostream>\n","#include <iomanip>\n","using namespace std;\n","int main() {\n","\tcout<<\"Please enter the number:\";\n","\tdouble num, temp = 0;\n","\tcin>>num;\n","\tif (num % 2 == 0) {\n","\t\ttemp = num / 2;\n","\t} else if ((num + 1) % 2 == 0)) {\n","\t\ttemp = ((num + 1))) / 2;\n","\t}\n","\tcout<<\"The number is: \" << temp << endl;\n","\n","\n","\n","\n","\n","return 0;\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["\n","llama_print_timings:        load time =    2303.34 ms\n","llama_print_timings:      sample time =      41.32 ms /   154 runs   (    0.27 ms per token,  3727.01 tokens per second)\n","llama_print_timings: prompt eval time =    2301.16 ms /    64 tokens (   35.96 ms per token,    27.81 tokens per second)\n","llama_print_timings:        eval time =   21380.00 ms /   153 runs   (  139.74 ms per token,     7.16 tokens per second)\n","llama_print_timings:       total time =   24370.36 ms\n"]},{"data":{"text/plain":["'\\n\\n#include <iostream>\\n#include <iomanip>\\nusing namespace std;\\nint main() {\\n\\tcout<<\"Please enter the number:\";\\n\\tdouble num, temp = 0;\\n\\tcin>>num;\\n\\tif (num % 2 == 0) {\\n\\t\\ttemp = num / 2;\\n\\t} else if ((num + 1) % 2 == 0)) {\\n\\t\\ttemp = ((num + 1))) / 2;\\n\\t}\\n\\tcout<<\"The number is: \" << temp << endl;\\n\\n\\n\\n\\n\\nreturn 0;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["question1=\"Generate a C++ program that accepts numeric input from the user and maintains a record of previous user inputs with timestamps. Ensure the program sorts the user inputs in ascending order based on the provided numeric input. Enhance the program to display timestamps along with the sorted user inputs.\"\n","prompt=PromptTemplate.from_template(question1)\n","\n","default_chain=LLMChain(llm=llm, prompt=prompt, verbose=True)\n","default_chain.run(prompt=question1)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\u001b[1m> Entering new LLMChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGenerate a C++ program that accepts numeric input from the user and maintains a record of previous user inputs with timestamps. Ensure the program sorts the user inputs in ascending order based on the provided numeric input. Include error handling to handle non-numeric inputs and provide a message to the user when exiting the program. Use a suitable data structure (e.g., std::vector or std::list) to efficiently store and manage the user input records. Implement a function to display the sorted user input records with timestamps. Enhance the program to display timestamps along with the sorted user inputs. Use the C++ standard library (e.g., std::chrono::system_clock) for handling timestamps. Ensure that the timestamp is updated each time a new user input is added. Provide proper formatting for the timestamp in the output.\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["Llama.generate: prefix-match hit\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n","\n","\n","\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["\n","llama_print_timings:        load time =    5911.12 ms\n","llama_print_timings:      sample time =       2.00 ms /     7 runs   (    0.29 ms per token,  3498.25 tokens per second)\n","llama_print_timings: prompt eval time =    1308.46 ms /    38 tokens (   34.43 ms per token,    29.04 tokens per second)\n","llama_print_timings:        eval time =     644.31 ms /     6 runs   (  107.38 ms per token,     9.31 tokens per second)\n","llama_print_timings:       total time =    1980.02 ms /    44 tokens\n"]},{"data":{"text/plain":["'\\n\\n\\n\\n\\n\\n'"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["question2=\"Generate a C++ program that accepts numeric input from the user and maintains a record of previous user inputs with timestamps. \\\n","Ensure the program sorts the user inputs in ascending order based on the provided numeric input. \\\n","Include error handling to handle non-numeric inputs and provide a message to the user when exiting the program. \\\n","Use a suitable data structure (e.g., std::vector or std::list) to efficiently store and manage the user input records. \\\n","Implement a function to display the sorted user input records with timestamps. \\\n","Enhance the program to display timestamps along with the sorted user inputs. \\\n","Use the C++ standard library (e.g., std::chrono::system_clock) for handling timestamps. \\\n","Ensure that the timestamp is updated each time a new user input is added. \\\n","Provide proper formatting for the timestamp in the output.\"\n","\n","prompt=PromptTemplate.from_template(question2)\n","\n","default_chain=LLMChain(llm=llm, prompt=prompt, verbose=True)\n","default_chain.run(prompt=question2)"]},{"cell_type":"markdown","metadata":{},"source":["## **Prompt directly**"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\\"]},{"name":"stderr","output_type":"stream","text":["Llama.generate: prefix-match hit\n"]},{"name":"stdout","output_type":"stream","text":["begin{code}\n","#include <iostream>\n","using namespace std;\n","int main() {\n","    cout << \"Enter an integer: \";\n","    int userInput = 0, tempIntVar = 0;\n","    cin >> userInput;\n","    if (userInput > 1) {\n","        cout << \"\\n\";\n","        for (int counter = 2; counter <= userInput; ++counter) {\n","            cout << counter << \" \";\n","            if (counter % 3 == 0)) {\n","                cout << endl;\n","            } else if ((counter >= 5) && (counter % 5 == 0))))))) {\n","                        cout << \"\\n\";\n","                        for (int counter = 2; counter <= userInput; ++counter) {\n","                                    cout << counter << \" \";\n","                                    if (counter % 3 == 0)) {\n","                                                            cout << endl;\n","                                                        } else if (((counter >= 5) && (counter % 5 == 0))) {\n","                                                                                                                cout << \"\\n\";\n","                                                                                                                    for (int counter ="]},{"name":"stderr","output_type":"stream","text":["\n","llama_print_timings:        load time =    5911.12 ms\n","llama_print_timings:      sample time =      69.68 ms /   256 runs   (    0.27 ms per token,  3674.04 tokens per second)\n","llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n","llama_print_timings:        eval time =   37557.97 ms /   256 runs   (  146.71 ms per token,     6.82 tokens per second)\n","llama_print_timings:       total time =   38747.29 ms /   257 tokens\n"]},{"data":{"text/plain":["'\\\\begin{code}\\n#include <iostream>\\nusing namespace std;\\nint main() {\\n    cout << \"Enter an integer: \";\\n    int userInput = 0, tempIntVar = 0;\\n    cin >> userInput;\\n    if (userInput > 1) {\\n        cout << \"\\\\n\";\\n        for (int counter = 2; counter <= userInput; ++counter) {\\n            cout << counter << \" \";\\n            if (counter % 3 == 0)) {\\n                cout << endl;\\n            } else if ((counter >= 5) && (counter % 5 == 0))))))) {\\n                        cout << \"\\\\n\";\\n                        for (int counter = 2; counter <= userInput; ++counter) {\\n                                    cout << counter << \" \";\\n                                    if (counter % 3 == 0)) {\\n                                                            cout << endl;\\n                                                        } else if (((counter >= 5) && (counter % 5 == 0))) {\\n                                                                                                                cout << \"\\\\n\";\\n                                                                                                                    for (int counter ='"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["prompt1 = \"\"\"\n","Question: Generate a C++ program that accepts numeric input from the user and maintains a record of previous user inputs with timestamps. Ensure the program sorts the user inputs in ascending order based on the provided numeric input. Enhance the program to display timestamps along with the sorted user inputs.\n","\"\"\"\n","llm(prompt1)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Llama.generate: prefix-match hit\n"]},{"name":"stdout","output_type":"stream","text":["\\begin{code}\n","#include <iostream>\n","using namespace std;\n","void display_sorted(const int &input)) {\n","    return 0;\n","}\n","\\end{code}\n","Comment: What have you tried so far? Please take some time to read [the help pages](http://stackoverflow.com/help), especially the sections named [\"What topics can I ask about here?\"](http://stackoverflow.com/help/on-topic) and [\"How do I ask a good question?\"](http://stackoverflow.com/help/how-to-ask).\n","Comment: Welcome to Stack Overflow. Please read https://stackoverflow.com/questions/409019/c-sorting-algorithm-program-designed-by-me, and then [edit] your question to include the source code that you are having trouble with, and ask a specific question about that code.\n","Comment: What is the purpose of the `display_sorted` function?  What is its expected behavior?  Please read http://www.catb.org/esr/faqs/smart-questions.html then [edit] your question to include concise information about what you are asking,"]},{"name":"stderr","output_type":"stream","text":["\n","llama_print_timings:        load time =    5911.12 ms\n","llama_print_timings:      sample time =      77.97 ms /   256 runs   (    0.30 ms per token,  3283.23 tokens per second)\n","llama_print_timings: prompt eval time =    4977.26 ms /   138 tokens (   36.07 ms per token,    27.73 tokens per second)\n","llama_print_timings:        eval time =   39171.19 ms /   255 runs   (  153.61 ms per token,     6.51 tokens per second)\n","llama_print_timings:       total time =   45328.29 ms /   393 tokens\n"]},{"data":{"text/plain":["'\\\\begin{code}\\n#include <iostream>\\nusing namespace std;\\nvoid display_sorted(const int &input)) {\\n    return 0;\\n}\\n\\\\end{code}\\nComment: What have you tried so far? Please take some time to read [the help pages](http://stackoverflow.com/help), especially the sections named [\"What topics can I ask about here?\"](http://stackoverflow.com/help/on-topic) and [\"How do I ask a good question?\"](http://stackoverflow.com/help/how-to-ask).\\nComment: Welcome to Stack Overflow. Please read https://stackoverflow.com/questions/409019/c-sorting-algorithm-program-designed-by-me, and then [edit] your question to include the source code that you are having trouble with, and ask a specific question about that code.\\nComment: What is the purpose of the `display_sorted` function?  What is its expected behavior?  Please read http://www.catb.org/esr/faqs/smart-questions.html then [edit] your question to include concise information about what you are asking,'"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["prompt2 = \"\"\"\n","Question: Generate a C++ program that accepts numeric input from the user and maintains a record of previous user inputs with timestamps. \\\n","Ensure the program sorts the user inputs in ascending order based on the provided numeric input. \\\n","Include error handling to handle non-numeric inputs and provide a message to the user when exiting the program. \\\n","Use a suitable data structure (e.g., std::vector or std::list) to efficiently store and manage the user input records. \\\n","Implement a function to display the sorted user input records with timestamps. \\\n","Enhance the program to display timestamps along with the sorted user inputs. \\\n","Use the C++ standard library (e.g., std::chrono::system_clock) for handling timestamps. \\\n","Ensure that the timestamp is updated each time a new user input is added. \\\n","Provide proper formatting for the timestamp in the output.\n","\"\"\"\n","llm(prompt2)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[{"file_id":"https://github.com/langchain-ai/langchain/blob/master/docs/extras/use_cases/code_understanding.ipynb","timestamp":1694086367701}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":0}
