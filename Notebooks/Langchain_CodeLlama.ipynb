{"cells":[{"cell_type":"markdown","metadata":{"id":"qMdW0KaMacQf"},"source":["### Open Source LLM\n","\n","We can combine LLamaCPP or [Ollama integrated](https://ollama.ai) to use [Code LLaMA](https://about.fb.com/news/2023/08/code-llama-ai-for-coding/) /blog/run-code-llama-locally）.\n","\n","Note：Please update `llama-cpp-python` , so that the `gguf` [format](https://github.com/abetlen/llama-cpp-python/pull/633) can be used here.\n","\n","````\n","CMAKE_ARGS =“-DLLAMA_METAL = on”FORCE_CMAKE = 1 /Users/rlm/miniforge3/envs/llama2/bin/pip install -U llama-cpp-python --no-cache-dir\n","````\n","\n","Check the lastest code-llama model [here](https://huggingface.co/TheBloke/CodeLlama-13B-Instruct-GGUF/tree/main)."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36579,"status":"ok","timestamp":1694145717564,"user":{"displayName":"MENGYANG LU","userId":"11275008728594910650"},"user_tz":-480},"id":"JK2AXJuQ8zN-","outputId":"bf913e92-674f-4cb3-84cf-516c291cce95"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting llama-cpp-python\n","  Downloading llama_cpp_python-0.2.28.tar.gz (9.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n","\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /srv/users/rudxia/anaconda3/envs/GPT4/lib/python3.8/site-packages (from llama-cpp-python) (4.9.0)\n","Requirement already satisfied: numpy>=1.20.0 in /srv/users/rudxia/anaconda3/envs/GPT4/lib/python3.8/site-packages (from llama-cpp-python) (1.24.4)\n","Collecting diskcache>=5.6.1 (from llama-cpp-python)\n","  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n","Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n","  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.28-cp38-cp38-manylinux_2_31_x86_64.whl size=2113205 sha256=b4ce51d3f00900add3524f1b5ce69bc7602159f986c6ca4e7bc86ca7090370cb\n","  Stored in directory: /srv/users/rudxia/.cache/pip/wheels/12/12/f8/9e08a6fefe1455c9f86713e5c834fa8cfc117c64ca82c53c25\n","Successfully built llama-cpp-python\n","Installing collected packages: diskcache, llama-cpp-python\n","Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.28\n"]}],"source":["!pip install -U llama-cpp-python\n","!pip install openai tiktoken chromadb langchain"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1694145633209,"user":{"displayName":"MENGYANG LU","userId":"11275008728594910650"},"user_tz":-480},"id":"I69RB2W_acQf"},"outputs":[],"source":["from langchain import PromptTemplate\n","from langchain.llms import LlamaCpp\n","from langchain.chains import LLMChain\n","from langchain.callbacks.manager import CallbackManager\n","from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":76376,"status":"ok","timestamp":1694145803915,"user":{"displayName":"MENGYANG LU","userId":"11275008728594910650"},"user_tz":-480},"id":"_0SIH6eqacQf","outputId":"d22c95a4-9de6-49ef-b659-fe49dd55e61c","scrolled":true},"outputs":[{"name":"stderr","output_type":"stream","text":["llama_model_loader: loaded meta data with 20 key-value pairs and 363 tensors from /srv/users/rudxia/Developer_MICCAI/CodeLlama/model/codellama-13b-instruct.Q3_K_M.gguf (version GGUF V2)\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.name str              = codellama_codellama-13b-instruct-hf\n","llama_model_loader: - kv   2:                       llama.context_length u32              = 16384\n","llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n","llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n","llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n","llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n","llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n","llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n","llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n","llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n","llama_model_loader: - kv  11:                          general.file_type u32              = 12\n","llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n","llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32016]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n","llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32016]   = [0.000000, 0.000000, 0.000000, 0.0000...\n","llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32016]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n","llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n","llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n","llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n","llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n","llama_model_loader: - type  f32:   81 tensors\n","llama_model_loader: - type q3_K:  161 tensors\n","llama_model_loader: - type q4_K:  116 tensors\n","llama_model_loader: - type q5_K:    4 tensors\n","llama_model_loader: - type q6_K:    1 tensors\n","llm_load_vocab: mismatch in special tokens definition ( 264/32016 vs 259/32016 ).\n","llm_load_print_meta: format           = GGUF V2\n","llm_load_print_meta: arch             = llama\n","llm_load_print_meta: vocab type       = SPM\n","llm_load_print_meta: n_vocab          = 32016\n","llm_load_print_meta: n_merges         = 0\n","llm_load_print_meta: n_ctx_train      = 16384\n","llm_load_print_meta: n_embd           = 5120\n","llm_load_print_meta: n_head           = 40\n","llm_load_print_meta: n_head_kv        = 40\n","llm_load_print_meta: n_layer          = 40\n","llm_load_print_meta: n_rot            = 128\n","llm_load_print_meta: n_embd_head_k    = 128\n","llm_load_print_meta: n_embd_head_v    = 128\n","llm_load_print_meta: n_gqa            = 1\n","llm_load_print_meta: n_embd_k_gqa     = 5120\n","llm_load_print_meta: n_embd_v_gqa     = 5120\n","llm_load_print_meta: f_norm_eps       = 0.0e+00\n","llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n","llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n","llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n","llm_load_print_meta: n_ff             = 13824\n","llm_load_print_meta: n_expert         = 0\n","llm_load_print_meta: n_expert_used    = 0\n","llm_load_print_meta: rope scaling     = linear\n","llm_load_print_meta: freq_base_train  = 1000000.0\n","llm_load_print_meta: freq_scale_train = 1\n","llm_load_print_meta: n_yarn_orig_ctx  = 16384\n","llm_load_print_meta: rope_finetuned   = unknown\n","llm_load_print_meta: model type       = 13B\n","llm_load_print_meta: model ftype      = Q3_K - Medium\n","llm_load_print_meta: model params     = 13.02 B\n","llm_load_print_meta: model size       = 5.90 GiB (3.89 BPW) \n","llm_load_print_meta: general.name     = codellama_codellama-13b-instruct-hf\n","llm_load_print_meta: BOS token        = 1 '<s>'\n","llm_load_print_meta: EOS token        = 2 '</s>'\n","llm_load_print_meta: UNK token        = 0 '<unk>'\n","llm_load_print_meta: LF token         = 13 '<0x0A>'\n","llm_load_tensors: ggml ctx size       =    0.14 MiB\n","llm_load_tensors: system memory used  = 6043.69 MiB\n","...................................................................................................\n","llama_new_context_with_model: n_ctx      = 100000\n","llama_new_context_with_model: freq_base  = 10000.0\n","llama_new_context_with_model: freq_scale = 1\n","llama_new_context_with_model: KV self size  = 78125.00 MiB, K (f16): 39062.50 MiB, V (f16): 39062.50 MiB\n","llama_build_graph: non-view tensors processed: 844/844\n","llama_new_context_with_model: compute buffer total size = 8041.00 MiB\n","AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"]}],"source":["callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n","llm = LlamaCpp(\n","    model_path=\"/srv/users/rudxia/Developer_MICCAI/CodeLlama/model/codellama-13b-instruct.Q3_K_M.gguf\",\n","    n_ctx=100000,\n","    n_gpu_layers=1,\n","    n_batch=512,\n","    f16_kv=False,  # MUST set to True, otherwise you will run into problem after a couple of calls\n","    callback_manager=callback_manager,\n","    verbose=True,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## **Prompt after formatting**"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\u001b[1m> Entering new LLMChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGenerate a C++ program that accepts numeric input from the user and maintains a record of previous user inputs with timestamps. Ensure the program sorts the user inputs in ascending order based on the provided numeric input. Enhance the program to display timestamps along with the sorted user inputs.\u001b[0m\n","\n","\n","#include <iostream>\n","#include <iomanip>\n","using namespace std;\n","int main() {\n","\tcout<<\"Please enter the number:\";\n","\tdouble num, temp = 0;\n","\tcin>>num;\n","\tif (num % 2 == 0) {\n","\t\ttemp = num / 2;\n","\t} else if ((num + 1) % 2 == 0)) {\n","\t\ttemp = ((num + 1))) / 2;\n","\t}\n","\tcout<<\"The number is: \" << temp << endl;\n","\n","\n","\n","\n","\n","return 0;\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["\n","llama_print_timings:        load time =    2303.34 ms\n","llama_print_timings:      sample time =      41.32 ms /   154 runs   (    0.27 ms per token,  3727.01 tokens per second)\n","llama_print_timings: prompt eval time =    2301.16 ms /    64 tokens (   35.96 ms per token,    27.81 tokens per second)\n","llama_print_timings:        eval time =   21380.00 ms /   153 runs   (  139.74 ms per token,     7.16 tokens per second)\n","llama_print_timings:       total time =   24370.36 ms\n"]},{"data":{"text/plain":["'\\n\\n#include <iostream>\\n#include <iomanip>\\nusing namespace std;\\nint main() {\\n\\tcout<<\"Please enter the number:\";\\n\\tdouble num, temp = 0;\\n\\tcin>>num;\\n\\tif (num % 2 == 0) {\\n\\t\\ttemp = num / 2;\\n\\t} else if ((num + 1) % 2 == 0)) {\\n\\t\\ttemp = ((num + 1))) / 2;\\n\\t}\\n\\tcout<<\"The number is: \" << temp << endl;\\n\\n\\n\\n\\n\\nreturn 0;\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["question=\"Generate a C++ program that accepts numeric input from the user and maintains a record of previous user inputs with timestamps. Ensure the program sorts the user inputs in ascending order based on the provided numeric input. Enhance the program to display timestamps along with the sorted user inputs.\"\n","prompt=PromptTemplate.from_template(question)\n","\n","default_chain=LLMChain(llm=llm, prompt=prompt, verbose=True)\n","default_chain.run(prompt=question)"]},{"cell_type":"markdown","metadata":{},"source":["## **Prompt directly**"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Comment: Please clarify your specific problem or provide additional details to highlight exactly what you need. As it's currently written, it's hard to tell exactly what you're asking.\n","Answer: \\begin{code}\n","#include <iostream>\n","using namespace std;\n","int main() {\n","    int num1 = 0;\n","    cout << \"Enter a number: \";\n","    cin >> num1;\n","    if (num1 > 0) {\n","        cout << \"\\nThe number is positive.\";\n","    } else if ((num1 < 0)) {\n","        cout << \"\\nThe number is negative.\";\n","    } else {\n","        cout << \"\\nThe number is zero.\";\n","    }\n","}\n","\\end{code}"]},{"name":"stderr","output_type":"stream","text":["\n","llama_print_timings:        load time =     309.13 ms\n","llama_print_timings:      sample time =      47.67 ms /   165 runs   (    0.29 ms per token,  3461.22 tokens per second)\n","llama_print_timings: prompt eval time =    2692.54 ms /    69 tokens (   39.02 ms per token,    25.63 tokens per second)\n","llama_print_timings:        eval time =   23341.78 ms /   164 runs   (  142.33 ms per token,     7.03 tokens per second)\n","llama_print_timings:       total time =   26691.10 ms\n"]},{"data":{"text/plain":["'Comment: Please clarify your specific problem or provide additional details to highlight exactly what you need. As it\\'s currently written, it\\'s hard to tell exactly what you\\'re asking.\\nAnswer: \\\\begin{code}\\n#include <iostream>\\nusing namespace std;\\nint main() {\\n    int num1 = 0;\\n    cout << \"Enter a number: \";\\n    cin >> num1;\\n    if (num1 > 0) {\\n        cout << \"\\\\nThe number is positive.\";\\n    } else if ((num1 < 0)) {\\n        cout << \"\\\\nThe number is negative.\";\\n    } else {\\n        cout << \"\\\\nThe number is zero.\";\\n    }\\n}\\n\\\\end{code}'"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["prompt = \"\"\"\n","Question: Generate a C++ program that accepts numeric input from the user and maintains a record of previous user inputs with timestamps. Ensure the program sorts the user inputs in ascending order based on the provided numeric input. Enhance the program to display timestamps along with the sorted user inputs.\n","\"\"\"\n","llm(prompt)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[{"file_id":"https://github.com/langchain-ai/langchain/blob/master/docs/extras/use_cases/code_understanding.ipynb","timestamp":1694086367701}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":0}
